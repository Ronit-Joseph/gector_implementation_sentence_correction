{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7410882e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronit/miniconda3/envs/gector_env/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ronit/miniconda3/envs/gector_env/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4047/4031645995.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create an instance of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGecBERTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/output_vocabulary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"./gector/roberta-large_1_pie_1bw_st3.th\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/gector_blade/text_correction_GECtor/new_folder_temp/gector_implementation_sentence_correction/gector/gec_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_path, model_paths, weigths, max_len, min_len, lowercase_tokens, log, iterations, model_name, special_tokens_fix, is_ensemble, min_error_probability, confidence, del_confidence, resolve_cycles)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_tokens_fix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mweights_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_weights_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_tokens_fix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             model = Seq2Labels(vocab=self.vocab,\n\u001b[1;32m     64\u001b[0m                                \u001b[0mtext_field_embedder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_embbeder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_tokens_fix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/gector_blade/text_correction_GECtor/new_folder_temp/gector_implementation_sentence_correction/gector/gec_model.py\u001b[0m in \u001b[0;36m_get_indexer\u001b[0;34m(self, weights_name, special_tokens_fix)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mdo_lowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mmax_pieces_per_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mspecial_tokens_fix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspecial_tokens_fix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'bert'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbert_token_indexer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/gector_blade/text_correction_GECtor/new_folder_temp/gector_implementation_sentence_correction/gector/tokenizer_indexer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pretrained_model, do_lowercase, max_pieces, max_pieces_per_token, special_tokens_fix)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         model_tokenizer = AutoTokenizer.from_pretrained(\n\u001b[0;32m--> 143\u001b[0;31m             model_name, do_lower_case=do_lowercase, do_basic_tokenize=False, use_fast=True)\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m# to adjust all tokenizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gector_env/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0mconfig_tokenizer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer_class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gector_env/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         )\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gector_env/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1408\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m         )\n\u001b[1;32m   1412\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gector_env/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1625\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m                     raise ValueError(\n\u001b[0;32m-> 1627\u001b[0;31m                         \u001b[0;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1628\u001b[0m                         \u001b[0;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m                     )\n",
      "\u001b[0;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from gector.gec_model import GecBERTModel\n",
    "\n",
    "# Create an instance of the model\n",
    "model = GecBERTModel(vocab_path = \"./data/output_vocabulary\", model_paths = [\"./gector/roberta-large_1_pie_1bw_st3.th\"])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0e2d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add the sentence with grammatical errors\n",
    "sent = 'hey guys hows it going my names george'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8f9081b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'guys', 'hows', 'it', 'going', 'my', 'names', 'george']\n",
      "['Hey', 'guys', ',', 'how', 'is', 'it', 'going', '?', 'My', 'name', 'is', 'George']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the words \n",
    "batch = []\n",
    "batch.append(sent.split())\n",
    "#temp printing batch-teh list of sentence with errors\n",
    "for i in range(len(batch)):\n",
    "    print(batch[i])\n",
    "#temp printing final batch - the list of corrected sentence  \n",
    "final_batch, total_updates = model.handle_batch(batch)\n",
    "#temp\n",
    "for i in range(len(final_batch)):\n",
    "    print(final_batch[i])\n",
    "    print(total_updates)\n",
    "#temp\n",
    "updated_sent = \" \".join(final_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ecba802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_dict(input_sentence, corrected_sentence):\n",
    "    output_dict_list = []\n",
    "    end_index = -2\n",
    "    i = 0\n",
    "    j = i\n",
    "    while i < len(input_sentence) and j<len(corrected_sentence):\n",
    "        original_word = input_sentence[i]\n",
    "        corrected_word = corrected_sentence[j]\n",
    "\n",
    "        if original_word.lower() == corrected_word.lower():\n",
    "            start_index = end_index + 2\n",
    "            end_index = start_index\n",
    "            end_index += len(corrected_word) \n",
    "            output_dict = {\n",
    "                'word': original_word,\n",
    "                'correction': corrected_word,\n",
    "                'start_index': start_index,\n",
    "                'end_index': end_index,\n",
    "            }\n",
    "            if(original_word != corrected_word):\n",
    "                output_dict_list.append(output_dict)\n",
    "            i += 1\n",
    "            j += 1\n",
    "            continue\n",
    "        \n",
    "        # Check up to 3 tokens in the corrected sentence for correction\n",
    "        correction_tokens = []\n",
    "        k = j+2\n",
    "        while j < min(k, len(corrected_sentence)):\n",
    "            correction_tokens.append(corrected_sentence[j])\n",
    "            if original_word.lower() == corrected_sentence[j].lower():\n",
    "                j += 1\n",
    "                break\n",
    "            j += 1\n",
    "            \n",
    "\n",
    "        correction = ' '.join(correction_tokens) if any(correction_tokens) else None\n",
    "\n",
    "        start_index = end_index\n",
    "        end_index = start_index + len(correction)\n",
    "\n",
    "        output_dict = {\n",
    "            'word': original_word,\n",
    "            'correction': correction,\n",
    "            'start_index': start_index,\n",
    "            'end_index': end_index,\n",
    "        }\n",
    "        output_dict_list.append(output_dict)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # while(i<len(input_sentence)):\n",
    "        \n",
    "    return output_dict_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7696ba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'hey', 'correction': 'Hey', 'start_index': 0, 'end_index': 3}\n",
      "{'word': 'hows', 'correction': ', how', 'start_index': 9, 'end_index': 14}\n",
      "{'word': 'it', 'correction': 'is it', 'start_index': 14, 'end_index': 19}\n",
      "{'word': 'my', 'correction': '? My', 'start_index': 26, 'end_index': 30}\n",
      "{'word': 'names', 'correction': 'name is', 'start_index': 30, 'end_index': 37}\n",
      "{'word': 'george', 'correction': 'George', 'start_index': 39, 'end_index': 45}\n"
     ]
    }
   ],
   "source": [
    "# input_sentence = ['hey', 'guys', 'hows', 'it', 'going', 'my', 'names', 'george']\n",
    "# corrected_sentence = ['Hey', 'guys', ',', 'how', 'is', 'it', 'going', '?', 'My', 'name', 'is', 'George']\n",
    "\n",
    "output_dict_list = generate_output_dict(batch[0], final_batch[0])\n",
    "\n",
    "# Print the output\n",
    "for output_dict in output_dict_list:\n",
    "    print(output_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd7d4474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: hey guys hows it going my names george\n",
      "\n",
      "Updated Sentence: Hey guys , how is it going ? My name is George\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original Sentence: {sent}\\n\")\n",
    "print(f\"Updated Sentence: {updated_sent}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gector_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
